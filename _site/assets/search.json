

[
  
  
    
    
  
  
  
  {
    "title": "Improving Performance of Vision Encoding Large Language Models with Contextual Prompts",
    "excerpt": "An exploration and navigation of using state-of-the-art Deep Learning and Large Language models to interpret X-Ray images and generate reports.\n",
    "content": "Overview\n\nThis is a a Data Science Capstone Project investigated and put together by Luke Taylor, Muchan Li, and Raymond Song under the mentorship of Albert Hsiao, MD, PhD. All are affiliated with the Halicioglu Data Science Institute at UC San Diego. Professor Hsiao is additionally affiliated with UC San Diego Health and Radiology Department.\n\n\n  Codebase&nbsp; github\n\n\n  Report 📝\n\n  Poster 🪧\n\n  Team Info 👨‍💻\n\n\n\nProblem Statement and Motivation\n\nDeep learning models like convolutional neurla networks (CNNs) have demonstrated promising ways of application in automating radiograph analysis, detecting conditions such as pulmonary edema, pneumothorax, and so forth. However, vast amount of data other than the radiographs has been left out, such radiologist reports, in the modeling process and can potentially make automation and predicition more accurate and usable. Inspired by advancements in Large Language Models (LLMs) and Vision Transformers (ViTs), we’re exploring multi-modal models that integrate text and image data for improved results.\n\n\n\nObjectives\n\n\n  Navigate how multi-modal models are used to leverage both X-Ray reports and images to generate reports.\n  Assess whether the generated reports mimic the style of reports written by the expert readers.\n  Assess the accuracy of the generated reports in identifying pathologies.\n    \n      Determine whether additional context in the text input of an LLM improves generated text outcomes.\n    \n  \n  Explore the effect of prompt engineering on report generation and accuracy.\n    \n      Evaluate and quantify the improvement in the LLM for Chest X-rays given more context.\n    \n  \n\n\nMethods\n\n\n  About 100K Chest Radiographs and their text reports from UCSD Health\n    \n      Among which, about 97000 images are used to fine-tune the model, and about 2000 images are reserved as the test set.\n    \n  \n  Fine-tuned a Large Language Model (LLM) with vision capabilities instead of training both a vision CNN tower and a LLM from scratch\n  Using Large Language and Vision Assistant v1.5 (LLaVA)[1][2][3] as the base model which is based on Vicuna 13B v1.5 and CLIP ViT-L/14 visual encoder\n  Input is an X-ray and then a prompt that may include information about the previous clinical history and additional patient context.\n  Ground truth output is actual radiologist report for the corresponding X-ray\n  The fine-tuning of the model was done using a Nvidia RTX A6000 with LORA adapters due to GPU memory constraints.\n  Evaluated with cosine similarity scores between bio term specific sentence transformer embeddings.\n  Extracted label probabilities for common lung conditions using Facebook BART zero shot classification to evaluate diagnostic accuracy.\n\n\nResults and Analysis\n\nSimilarity Between Generated Reports and Ground Truth\n\n\nClick on each radiologist’s name below to learn model’s performance with respect to each individual:\n\n  👨‍⚕️ Dr. Seth Kligerman\n  \n\n\n  👨‍⚕️ Dr. Lewis Hahn\n  \n\n\n  👨‍⚕️ Dr. Michael Horowitz\n  \n\n\n  👨‍⚕️ Dr. Ravi Rajpoot\n  \n\n\n  👩‍⚕️ Dr. Sharon Brouha\n  \n\n\n  👩‍⚕️ Dr. Kathleen Jacobs\n  \n\n\n  👩‍⚕️ Dr. Elizabeth Weihe\n  \n\n\n  👨‍⚕️ Dr. Albert Hsiao\n  \n\n\n  👨‍⚕️ Dr. William Ladd\n  \n\n\n  👨‍⚕️ Dr. Andrew Yen\n  \n\n\nPathology Detection Outcomes\n\n\n\n\n\n\nFuture Directions\n\nReferences\n",
    "url": "/"
  },
  
  {
    "title": "About Us",
    "excerpt": "\n",
    "content": "\nMentor\n\n\n    \n    \n    \n    \n        \n        \n        Albert Hsiao MD, PhD\n        \n        \n        \n        \n        Email contact: a3hsiao[at]health[dot]ucsd[dot]edu\n        \n\n        Dr. Hsiao is awesome. His full profile is linked above.\n\n    \n\n\nStudents\n\n\n    \n    \n    \n    \n    \n    \n        \n        \n        Luke Taylor\n        \n        \n        \n        \n        Email contact: lwtaylor[at]ucsd[dot]edu\n        \n\n        \n\n    \n\n\n    \n    \n    \n    \n    \n    \n        \n        \n        Muchan Li\n        \n        \n        \n        \n        Email contact: mul005[at]ucsd[dot]edu\n        \n\n        Muchan Li is a senior Data Science student at UC San Diego that’s vastly interested in data, security &amp; privacy, and HCI. Upon graduation, he plans on continuing his academic career by pursuing a PhD and further investigating issues and solutions with respect to smart homes, smart home health, and usable security and privacy.\n\n    \n\n\n    \n    \n    \n    \n    \n    \n        \n        \n        Raymond Song\n        \n        \n        \n        \n        Email contact: rysong[at]ucsd[dot]edu\n        \n\n        \n\n    \n\n\n    \n\n",
    "url": "/people/"
  }
  
]

